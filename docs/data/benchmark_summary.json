{
  "generated_at_utc": "2026-02-26T15:26:18.289605+00:00",
  "title": "PhyloBioBixBench-Verified-50 Results",
  "totals": {
    "rows": 550,
    "repeats": 11,
    "questions": 50,
    "task_groups": 32
  },
  "source_files": [
    {
      "filename": "phylobiobixbench-verified-50-20260224-190546.csv",
      "file_code": "190546",
      "date": "2026-02-24",
      "rows": 250,
      "repeats": 5
    },
    {
      "filename": "phylobiobixbench-verified-50-20260224-224841.csv",
      "file_code": "224841",
      "date": "2026-02-24",
      "rows": 100,
      "repeats": 2
    },
    {
      "filename": "phylobiobixbench-verified-50-20260225-085809.csv",
      "file_code": "085809",
      "date": "2026-02-25",
      "rows": 200,
      "repeats": 4
    }
  ],
  "overall": {
    "direct": {
      "label": "Direct",
      "correct": 392,
      "total": 550,
      "pct": 71.3,
      "ci_95": {
        "lo": 67.4,
        "hi": 74.9
      }
    },
    "mcq_with_refusal": {
      "label": "MCQ with refusal",
      "correct": 468,
      "total": 550,
      "pct": 85.1,
      "ci_95": {
        "lo": 81.9,
        "hi": 87.8
      }
    },
    "mcq_without_refusal": {
      "label": "MCQ without refusal",
      "correct": 495,
      "total": 550,
      "pct": 90.0,
      "ci_95": {
        "lo": 87.2,
        "hi": 92.2
      }
    }
  },
  "by_file": [
    {
      "filename": "phylobiobixbench-verified-50-20260224-190546.csv",
      "file_code": "190546",
      "rows": 250,
      "repeats": 5,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 176,
          "total": 250,
          "pct": 70.4,
          "ci_95": {
            "lo": 64.5,
            "hi": 75.7
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 209,
          "total": 250,
          "pct": 83.6,
          "ci_95": {
            "lo": 78.5,
            "hi": 87.7
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 223,
          "total": 250,
          "pct": 89.2,
          "ci_95": {
            "lo": 84.7,
            "hi": 92.5
          }
        }
      },
      "latency": {
        "mean_ms": 908706.5,
        "p95_ms": 1737829.1
      }
    },
    {
      "filename": "phylobiobixbench-verified-50-20260224-224841.csv",
      "file_code": "224841",
      "rows": 100,
      "repeats": 2,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 76,
          "total": 100,
          "pct": 76.0,
          "ci_95": {
            "lo": 66.8,
            "hi": 83.3
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 87,
          "total": 100,
          "pct": 87.0,
          "ci_95": {
            "lo": 79.0,
            "hi": 92.2
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 89,
          "total": 100,
          "pct": 89.0,
          "ci_95": {
            "lo": 81.4,
            "hi": 93.7
          }
        }
      },
      "latency": {
        "mean_ms": 848748.2,
        "p95_ms": 1555561.3
      }
    },
    {
      "filename": "phylobiobixbench-verified-50-20260225-085809.csv",
      "file_code": "085809",
      "rows": 200,
      "repeats": 4,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 140,
          "total": 200,
          "pct": 70.0,
          "ci_95": {
            "lo": 63.3,
            "hi": 75.9
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 172,
          "total": 200,
          "pct": 86.0,
          "ci_95": {
            "lo": 80.5,
            "hi": 90.1
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 183,
          "total": 200,
          "pct": 91.5,
          "ci_95": {
            "lo": 86.8,
            "hi": 94.6
          }
        }
      },
      "latency": {
        "mean_ms": 936134.0,
        "p95_ms": 1742421.8
      }
    }
  ],
  "by_repeat": [
    {
      "file_code": "085809",
      "repeat_index": 3,
      "repeat_label": "r3",
      "run_id": "phylobiobixbench-verified-50-20260225-085809-r3",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 31,
          "total": 50,
          "pct": 62.0,
          "ci_95": {
            "lo": 48.2,
            "hi": 74.1
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 45,
          "total": 50,
          "pct": 90.0,
          "ci_95": {
            "lo": 78.6,
            "hi": 95.7
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 48,
          "total": 50,
          "pct": 96.0,
          "ci_95": {
            "lo": 86.5,
            "hi": 98.9
          }
        }
      },
      "rank_mcq_without_refusal": 1
    },
    {
      "file_code": "190546",
      "repeat_index": 1,
      "repeat_label": "r1",
      "run_id": "phylobiobixbench-verified-50-20260224-190546-r1",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 36,
          "total": 50,
          "pct": 72.0,
          "ci_95": {
            "lo": 58.3,
            "hi": 82.5
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 45,
          "total": 50,
          "pct": 90.0,
          "ci_95": {
            "lo": 78.6,
            "hi": 95.7
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 47,
          "total": 50,
          "pct": 94.0,
          "ci_95": {
            "lo": 83.8,
            "hi": 97.9
          }
        }
      },
      "rank_mcq_without_refusal": 2
    },
    {
      "file_code": "190546",
      "repeat_index": 2,
      "repeat_label": "r2",
      "run_id": "phylobiobixbench-verified-50-20260224-190546-r2",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 37,
          "total": 50,
          "pct": 74.0,
          "ci_95": {
            "lo": 60.4,
            "hi": 84.1
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 41,
          "total": 50,
          "pct": 82.0,
          "ci_95": {
            "lo": 69.2,
            "hi": 90.2
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 46,
          "total": 50,
          "pct": 92.0,
          "ci_95": {
            "lo": 81.2,
            "hi": 96.8
          }
        }
      },
      "rank_mcq_without_refusal": 3
    },
    {
      "file_code": "190546",
      "repeat_index": 3,
      "repeat_label": "r3",
      "run_id": "phylobiobixbench-verified-50-20260224-190546-r3",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 37,
          "total": 50,
          "pct": 74.0,
          "ci_95": {
            "lo": 60.4,
            "hi": 84.1
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 44,
          "total": 50,
          "pct": 88.0,
          "ci_95": {
            "lo": 76.2,
            "hi": 94.4
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 46,
          "total": 50,
          "pct": 92.0,
          "ci_95": {
            "lo": 81.2,
            "hi": 96.8
          }
        }
      },
      "rank_mcq_without_refusal": 4
    },
    {
      "file_code": "085809",
      "repeat_index": 1,
      "repeat_label": "r1",
      "run_id": "phylobiobixbench-verified-50-20260225-085809-r1",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 39,
          "total": 50,
          "pct": 78.0,
          "ci_95": {
            "lo": 64.8,
            "hi": 87.2
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 44,
          "total": 50,
          "pct": 88.0,
          "ci_95": {
            "lo": 76.2,
            "hi": 94.4
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 45,
          "total": 50,
          "pct": 90.0,
          "ci_95": {
            "lo": 78.6,
            "hi": 95.7
          }
        }
      },
      "rank_mcq_without_refusal": 5
    },
    {
      "file_code": "224841",
      "repeat_index": 1,
      "repeat_label": "r1",
      "run_id": "phylobiobixbench-verified-50-20260224-224841-r1",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 38,
          "total": 50,
          "pct": 76.0,
          "ci_95": {
            "lo": 62.6,
            "hi": 85.7
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 44,
          "total": 50,
          "pct": 88.0,
          "ci_95": {
            "lo": 76.2,
            "hi": 94.4
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 45,
          "total": 50,
          "pct": 90.0,
          "ci_95": {
            "lo": 78.6,
            "hi": 95.7
          }
        }
      },
      "rank_mcq_without_refusal": 6
    },
    {
      "file_code": "085809",
      "repeat_index": 2,
      "repeat_label": "r2",
      "run_id": "phylobiobixbench-verified-50-20260225-085809-r2",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 36,
          "total": 50,
          "pct": 72.0,
          "ci_95": {
            "lo": 58.3,
            "hi": 82.5
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 43,
          "total": 50,
          "pct": 86.0,
          "ci_95": {
            "lo": 73.8,
            "hi": 93.0
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 45,
          "total": 50,
          "pct": 90.0,
          "ci_95": {
            "lo": 78.6,
            "hi": 95.7
          }
        }
      },
      "rank_mcq_without_refusal": 7
    },
    {
      "file_code": "085809",
      "repeat_index": 4,
      "repeat_label": "r4",
      "run_id": "phylobiobixbench-verified-50-20260225-085809-r4",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 34,
          "total": 50,
          "pct": 68.0,
          "ci_95": {
            "lo": 54.2,
            "hi": 79.2
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 40,
          "total": 50,
          "pct": 80.0,
          "ci_95": {
            "lo": 67.0,
            "hi": 88.8
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 45,
          "total": 50,
          "pct": 90.0,
          "ci_95": {
            "lo": 78.6,
            "hi": 95.7
          }
        }
      },
      "rank_mcq_without_refusal": 8
    },
    {
      "file_code": "224841",
      "repeat_index": 2,
      "repeat_label": "r2",
      "run_id": "phylobiobixbench-verified-50-20260224-224841-r2",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 38,
          "total": 50,
          "pct": 76.0,
          "ci_95": {
            "lo": 62.6,
            "hi": 85.7
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 43,
          "total": 50,
          "pct": 86.0,
          "ci_95": {
            "lo": 73.8,
            "hi": 93.0
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 44,
          "total": 50,
          "pct": 88.0,
          "ci_95": {
            "lo": 76.2,
            "hi": 94.4
          }
        }
      },
      "rank_mcq_without_refusal": 9
    },
    {
      "file_code": "190546",
      "repeat_index": 5,
      "repeat_label": "r5",
      "run_id": "phylobiobixbench-verified-50-20260224-190546-r5",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 33,
          "total": 50,
          "pct": 66.0,
          "ci_95": {
            "lo": 52.2,
            "hi": 77.6
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 39,
          "total": 50,
          "pct": 78.0,
          "ci_95": {
            "lo": 64.8,
            "hi": 87.2
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 43,
          "total": 50,
          "pct": 86.0,
          "ci_95": {
            "lo": 73.8,
            "hi": 93.0
          }
        }
      },
      "rank_mcq_without_refusal": 10
    },
    {
      "file_code": "190546",
      "repeat_index": 4,
      "repeat_label": "r4",
      "run_id": "phylobiobixbench-verified-50-20260224-190546-r4",
      "rows": 50,
      "metrics": {
        "direct": {
          "label": "Direct",
          "correct": 33,
          "total": 50,
          "pct": 66.0,
          "ci_95": {
            "lo": 52.2,
            "hi": 77.6
          }
        },
        "mcq_with_refusal": {
          "label": "MCQ with refusal",
          "correct": 40,
          "total": 50,
          "pct": 80.0,
          "ci_95": {
            "lo": 67.0,
            "hi": 88.8
          }
        },
        "mcq_without_refusal": {
          "label": "MCQ without refusal",
          "correct": 41,
          "total": 50,
          "pct": 82.0,
          "ci_95": {
            "lo": 69.2,
            "hi": 90.2
          }
        }
      },
      "rank_mcq_without_refusal": 11
    }
  ],
  "question_scores": [
    {
      "question_id": "bix-32-q2",
      "task_group": "bix-32",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 0.0,
      "mcq_without_refusal_pct": 0.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 11
    },
    {
      "question_id": "bix-16-q1",
      "task_group": "bix-16",
      "n": 11,
      "direct_pct": 9.1,
      "mcq_with_refusal_pct": 9.1,
      "mcq_without_refusal_pct": 9.1,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 10
    },
    {
      "question_id": "bix-16-q3",
      "task_group": "bix-16",
      "n": 11,
      "direct_pct": 9.1,
      "mcq_with_refusal_pct": 9.1,
      "mcq_without_refusal_pct": 9.1,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 10
    },
    {
      "question_id": "bix-26-q5",
      "task_group": "bix-26",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 0.0,
      "mcq_without_refusal_pct": 45.5,
      "refusal_gap_pp": 45.5,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 6
    },
    {
      "question_id": "bix-61-q5",
      "task_group": "bix-61",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 18.2,
      "mcq_without_refusal_pct": 45.5,
      "refusal_gap_pp": 27.3,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 6
    },
    {
      "question_id": "bix-24-q2",
      "task_group": "bix-24",
      "n": 11,
      "direct_pct": 63.6,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 63.6,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 4
    },
    {
      "question_id": "bix-12-q4",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 81.8,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 2
    },
    {
      "question_id": "bix-17-q2",
      "task_group": "bix-17",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 81.8,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 2
    },
    {
      "question_id": "bix-12-q6",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": 9.1,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 1
    },
    {
      "question_id": "bix-20-q3",
      "task_group": "bix-20",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": -9.1,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 1
    },
    {
      "question_id": "bix-28-q3",
      "task_group": "bix-28",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 1
    },
    {
      "question_id": "bix-46-q4",
      "task_group": "bix-46",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 1
    },
    {
      "question_id": "bix-11-q1",
      "task_group": "bix-11",
      "n": 11,
      "direct_pct": 54.5,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-11-q2",
      "task_group": "bix-11",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-12-q2",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 9.1,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-12-q5",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-14-q1",
      "task_group": "bix-14",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 9.1,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-16-q4",
      "task_group": "bix-16",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 18.2,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-18-q1",
      "task_group": "bix-18",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-18-q3",
      "task_group": "bix-18",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-22-q1",
      "task_group": "bix-22",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-22-q4",
      "task_group": "bix-22",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-26-q3",
      "task_group": "bix-26",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-27-q5",
      "task_group": "bix-27",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-30-q3",
      "task_group": "bix-30",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-31-q2",
      "task_group": "bix-31",
      "n": 11,
      "direct_pct": 72.7,
      "mcq_with_refusal_pct": 72.7,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 27.3,
      "mcq_lift_pp": 27.3,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-34-q2",
      "task_group": "bix-34",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 18.2,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-34-q5",
      "task_group": "bix-34",
      "n": 11,
      "direct_pct": 54.5,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 18.2,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-35-q1",
      "task_group": "bix-35",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-35-q2",
      "task_group": "bix-35",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-37-q1",
      "task_group": "bix-37",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-37-q4",
      "task_group": "bix-37",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-38-q1",
      "task_group": "bix-38",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-41-q5",
      "task_group": "bix-41",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-43-q2",
      "task_group": "bix-43",
      "n": 11,
      "direct_pct": 63.6,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 36.4,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-43-q4",
      "task_group": "bix-43",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-45-q1",
      "task_group": "bix-45",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 18.2,
      "mcq_lift_pp": 18.2,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-47-q3",
      "task_group": "bix-47",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-49-q4",
      "task_group": "bix-49",
      "n": 11,
      "direct_pct": 36.4,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 36.4,
      "mcq_lift_pp": 63.6,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-51-q2",
      "task_group": "bix-51",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-51-q8",
      "task_group": "bix-51",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-52-q2",
      "task_group": "bix-52",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-52-q6",
      "task_group": "bix-52",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-52-q7",
      "task_group": "bix-52",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-53-q2",
      "task_group": "bix-53",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-53-q5",
      "task_group": "bix-53",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-54-q7",
      "task_group": "bix-54",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 36.4,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-55-q1",
      "task_group": "bix-55",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-6-q4",
      "task_group": "bix-6",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-61-q2",
      "task_group": "bix-61",
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 0
    }
  ],
  "task_group_scores": [
    {
      "task_group": "bix-32",
      "questions": 1,
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 0.0,
      "mcq_without_refusal_pct": 0.0
    },
    {
      "task_group": "bix-16",
      "questions": 3,
      "n": 33,
      "direct_pct": 36.4,
      "mcq_with_refusal_pct": 33.3,
      "mcq_without_refusal_pct": 39.4
    },
    {
      "task_group": "bix-24",
      "questions": 1,
      "n": 11,
      "direct_pct": 63.6,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 63.6
    },
    {
      "task_group": "bix-26",
      "questions": 2,
      "n": 22,
      "direct_pct": 45.5,
      "mcq_with_refusal_pct": 50.0,
      "mcq_without_refusal_pct": 72.7
    },
    {
      "task_group": "bix-61",
      "questions": 2,
      "n": 22,
      "direct_pct": 50.0,
      "mcq_with_refusal_pct": 59.1,
      "mcq_without_refusal_pct": 72.7
    },
    {
      "task_group": "bix-17",
      "questions": 1,
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 81.8
    },
    {
      "task_group": "bix-20",
      "questions": 1,
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 90.9
    },
    {
      "task_group": "bix-28",
      "questions": 1,
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 90.9
    },
    {
      "task_group": "bix-46",
      "questions": 1,
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 90.9
    },
    {
      "task_group": "bix-12",
      "questions": 4,
      "n": 44,
      "direct_pct": 65.9,
      "mcq_with_refusal_pct": 88.6,
      "mcq_without_refusal_pct": 93.2
    },
    {
      "task_group": "bix-11",
      "questions": 2,
      "n": 22,
      "direct_pct": 27.3,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-14",
      "questions": 1,
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-18",
      "questions": 2,
      "n": 22,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-22",
      "questions": 2,
      "n": 22,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-27",
      "questions": 1,
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-30",
      "questions": 1,
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-31",
      "questions": 1,
      "n": 11,
      "direct_pct": 72.7,
      "mcq_with_refusal_pct": 72.7,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-34",
      "questions": 2,
      "n": 22,
      "direct_pct": 68.2,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-35",
      "questions": 2,
      "n": 22,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-37",
      "questions": 2,
      "n": 22,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-38",
      "questions": 1,
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-41",
      "questions": 1,
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-43",
      "questions": 2,
      "n": 22,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-45",
      "questions": 1,
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-47",
      "questions": 1,
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-49",
      "questions": 1,
      "n": 11,
      "direct_pct": 36.4,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-51",
      "questions": 2,
      "n": 22,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-52",
      "questions": 3,
      "n": 33,
      "direct_pct": 97.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-53",
      "questions": 2,
      "n": 22,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-54",
      "questions": 1,
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-55",
      "questions": 1,
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    },
    {
      "task_group": "bix-6",
      "questions": 1,
      "n": 11,
      "direct_pct": 100.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0
    }
  ],
  "hardest_questions": [
    {
      "question_id": "bix-32-q2",
      "task_group": "bix-32",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 0.0,
      "mcq_without_refusal_pct": 0.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 11
    },
    {
      "question_id": "bix-16-q1",
      "task_group": "bix-16",
      "n": 11,
      "direct_pct": 9.1,
      "mcq_with_refusal_pct": 9.1,
      "mcq_without_refusal_pct": 9.1,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 10
    },
    {
      "question_id": "bix-16-q3",
      "task_group": "bix-16",
      "n": 11,
      "direct_pct": 9.1,
      "mcq_with_refusal_pct": 9.1,
      "mcq_without_refusal_pct": 9.1,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 10
    },
    {
      "question_id": "bix-26-q5",
      "task_group": "bix-26",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 0.0,
      "mcq_without_refusal_pct": 45.5,
      "refusal_gap_pp": 45.5,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 6
    },
    {
      "question_id": "bix-61-q5",
      "task_group": "bix-61",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 18.2,
      "mcq_without_refusal_pct": 45.5,
      "refusal_gap_pp": 27.3,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 6
    },
    {
      "question_id": "bix-24-q2",
      "task_group": "bix-24",
      "n": 11,
      "direct_pct": 63.6,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 63.6,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 4
    },
    {
      "question_id": "bix-12-q4",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 81.8,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 2
    },
    {
      "question_id": "bix-17-q2",
      "task_group": "bix-17",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 81.8,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 2
    },
    {
      "question_id": "bix-12-q6",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": 9.1,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 1
    },
    {
      "question_id": "bix-20-q3",
      "task_group": "bix-20",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": -9.1,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 1
    },
    {
      "question_id": "bix-28-q3",
      "task_group": "bix-28",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 0.0,
      "mcq_without_refusal_failures": 1
    },
    {
      "question_id": "bix-46-q4",
      "task_group": "bix-46",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 1
    }
  ],
  "refusal_gap_questions": [
    {
      "question_id": "bix-26-q5",
      "task_group": "bix-26",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 0.0,
      "mcq_without_refusal_pct": 45.5,
      "refusal_gap_pp": 45.5,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 6
    },
    {
      "question_id": "bix-49-q4",
      "task_group": "bix-49",
      "n": 11,
      "direct_pct": 36.4,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 36.4,
      "mcq_lift_pp": 63.6,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-54-q7",
      "task_group": "bix-54",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 36.4,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-31-q2",
      "task_group": "bix-31",
      "n": 11,
      "direct_pct": 72.7,
      "mcq_with_refusal_pct": 72.7,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 27.3,
      "mcq_lift_pp": 27.3,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-61-q5",
      "task_group": "bix-61",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 18.2,
      "mcq_without_refusal_pct": 45.5,
      "refusal_gap_pp": 27.3,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 6
    },
    {
      "question_id": "bix-16-q4",
      "task_group": "bix-16",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 18.2,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-34-q5",
      "task_group": "bix-34",
      "n": 11,
      "direct_pct": 54.5,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 18.2,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-45-q1",
      "task_group": "bix-45",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 18.2,
      "mcq_lift_pp": 18.2,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-12-q2",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 9.1,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-12-q6",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 81.8,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 90.9,
      "refusal_gap_pp": 9.1,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 1
    },
    {
      "question_id": "bix-14-q1",
      "task_group": "bix-14",
      "n": 11,
      "direct_pct": 90.9,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 9.1,
      "mcq_lift_pp": 9.1,
      "mcq_without_refusal_failures": 0
    }
  ],
  "rescue": {
    "direct_wrong": 158,
    "direct_right": 392,
    "rescued": 103,
    "rescued_pct": 65.2,
    "lost": 0,
    "lost_pct": 0.0
  },
  "rescue_lifts": [
    {
      "question_id": "bix-11-q2",
      "task_group": "bix-11",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-12-q2",
      "task_group": "bix-12",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 90.9,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 9.1,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-27-q5",
      "task_group": "bix-27",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-38-q1",
      "task_group": "bix-38",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-54-q7",
      "task_group": "bix-54",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 36.4,
      "mcq_lift_pp": 100.0,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-49-q4",
      "task_group": "bix-49",
      "n": 11,
      "direct_pct": 36.4,
      "mcq_with_refusal_pct": 63.6,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 36.4,
      "mcq_lift_pp": 63.6,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-11-q1",
      "task_group": "bix-11",
      "n": 11,
      "direct_pct": 54.5,
      "mcq_with_refusal_pct": 100.0,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 0.0,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-26-q5",
      "task_group": "bix-26",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 0.0,
      "mcq_without_refusal_pct": 45.5,
      "refusal_gap_pp": 45.5,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 6
    },
    {
      "question_id": "bix-34-q5",
      "task_group": "bix-34",
      "n": 11,
      "direct_pct": 54.5,
      "mcq_with_refusal_pct": 81.8,
      "mcq_without_refusal_pct": 100.0,
      "refusal_gap_pp": 18.2,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 0
    },
    {
      "question_id": "bix-61-q5",
      "task_group": "bix-61",
      "n": 11,
      "direct_pct": 0.0,
      "mcq_with_refusal_pct": 18.2,
      "mcq_without_refusal_pct": 45.5,
      "refusal_gap_pp": 27.3,
      "mcq_lift_pp": 45.5,
      "mcq_without_refusal_failures": 6
    }
  ],
  "perfect_rescues": 5,
  "consistency": {
    "always_correct": 38,
    "always_wrong": 1,
    "mixed": 11,
    "always_correct_pct": 76.0,
    "always_wrong_pct": 2.0,
    "mixed_pct": 22.0,
    "mixed_questions": [
      "bix-12-q4",
      "bix-12-q6",
      "bix-16-q1",
      "bix-16-q3",
      "bix-17-q2",
      "bix-20-q3",
      "bix-24-q2",
      "bix-26-q5",
      "bix-28-q3",
      "bix-46-q4",
      "bix-61-q5"
    ]
  },
  "cross_run_variability": [
    {
      "question_id": "bix-12-q4",
      "by_file": {
        "085809": {
          "correct": 3,
          "total": 4
        },
        "190546": {
          "correct": 4,
          "total": 5
        },
        "224841": {
          "correct": 2,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-12-q6",
      "by_file": {
        "085809": {
          "correct": 4,
          "total": 4
        },
        "190546": {
          "correct": 4,
          "total": 5
        },
        "224841": {
          "correct": 2,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-16-q1",
      "by_file": {
        "085809": {
          "correct": 1,
          "total": 4
        },
        "190546": {
          "correct": 0,
          "total": 5
        },
        "224841": {
          "correct": 0,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-16-q3",
      "by_file": {
        "085809": {
          "correct": 1,
          "total": 4
        },
        "190546": {
          "correct": 0,
          "total": 5
        },
        "224841": {
          "correct": 0,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-17-q2",
      "by_file": {
        "085809": {
          "correct": 3,
          "total": 4
        },
        "190546": {
          "correct": 4,
          "total": 5
        },
        "224841": {
          "correct": 2,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-20-q3",
      "by_file": {
        "085809": {
          "correct": 4,
          "total": 4
        },
        "190546": {
          "correct": 4,
          "total": 5
        },
        "224841": {
          "correct": 2,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-24-q2",
      "by_file": {
        "085809": {
          "correct": 2,
          "total": 4
        },
        "190546": {
          "correct": 3,
          "total": 5
        },
        "224841": {
          "correct": 2,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-26-q5",
      "by_file": {
        "085809": {
          "correct": 3,
          "total": 4
        },
        "190546": {
          "correct": 2,
          "total": 5
        },
        "224841": {
          "correct": 0,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-28-q3",
      "by_file": {
        "085809": {
          "correct": 3,
          "total": 4
        },
        "190546": {
          "correct": 5,
          "total": 5
        },
        "224841": {
          "correct": 2,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-46-q4",
      "by_file": {
        "085809": {
          "correct": 4,
          "total": 4
        },
        "190546": {
          "correct": 5,
          "total": 5
        },
        "224841": {
          "correct": 1,
          "total": 2
        }
      }
    },
    {
      "question_id": "bix-61-q5",
      "by_file": {
        "085809": {
          "correct": 3,
          "total": 4
        },
        "190546": {
          "correct": 2,
          "total": 5
        },
        "224841": {
          "correct": 0,
          "total": 2
        }
      }
    }
  ],
  "file_codes": [
    "085809",
    "190546",
    "224841"
  ],
  "majority_vote": {
    "direct": {
      "correct": 39,
      "total": 50,
      "pct": 78.0
    },
    "mcq_with_refusal": {
      "correct": 45,
      "total": 50,
      "pct": 90.0
    },
    "mcq_without_refusal": {
      "correct": 45,
      "total": 50,
      "pct": 90.0
    }
  },
  "headline": {
    "mcq_lift_pp": 18.7,
    "refusal_gap_pp": 4.9,
    "best_repeat_pct": 96.0,
    "best_repeat_label": "085809-r3",
    "worst_repeat_pct": 82.0,
    "worst_repeat_label": "190546-r4",
    "repeat_spread_pp": 14.0,
    "repeat_median_pct": 90.0,
    "task_groups_at_100": 22,
    "task_groups_at_100_pct": 68.8,
    "total_task_groups": 32
  },
  "commentary": {
    "grading_modes": [
      {
        "name": "Direct",
        "description": "The model's free-form answer is graded by an LLM judge (gpt-5) against the ground truth. Sensitive to phrasing and formatting.",
        "accent": "coral"
      },
      {
        "name": "MCQ without refusal",
        "description": "The model's answer is mapped to the closest multiple-choice option (via gpt-4o), then graded for correctness. The model must select an option.",
        "accent": "teal"
      },
      {
        "name": "MCQ with refusal",
        "description": "Same as above, but the model is allowed to select \"Insufficient information to determine\" when the answer doesn't closely match any option. This is the stricter evaluator.",
        "accent": "sun"
      }
    ],
    "invariant_note": "Key invariant: mcq_with_refusal <= mcq_without_refusal. If the with-refusal mode selects a concrete option, that option must be correct (meaning without-refusal would also be correct). The with-refusal mode may additionally refuse when the answer is not precise enough.",
    "overall_interpretation": "MCQ without refusal provides a +18.7pp lift over direct grading. MCQ with refusal sits 4.9pp below MCQ without refusal, reflecting its stricter tolerance. The gap between direct and MCQ suggests that a significant portion of \"wrong\" answers under direct grading are actually correct answers that the direct grader cannot parse due to formatting differences.",
    "rescue_interpretation": "MCQ grading is purely additive \u2014 it never downgrades a correct direct answer, and it rescues 65.2% of direct failures.",
    "refusal_gap_interpretation": "The 4.9pp refusal gap (90.0% \u2192 85.1%) represents answer precision, not knowledge. The model often knows the right direction but its exact numerical or categorical answers aren't always tight enough for the stricter evaluator.",
    "strengths": [
      "90.0% MCQ w/o refusal is a strong result. The model demonstrates solid bioinformatics knowledge across the majority of the benchmark.",
      "38/50 questions (76%) are perfectly consistent. The model reliably gets these right every single time across all 11 repeats.",
      "22/32 task groups score 100%. The model handles the majority of bioinformatics analysis tasks flawlessly.",
      "MCQ rescue rate of 65.2% with zero losses. The MCQ format is a reliable safety net that recovers misgraded answers without ever introducing new errors.",
      "5 questions with perfect 100pp rescue. The model knows the answer to these questions every time \u2014 the issue is purely in how the direct grader interprets the response format.",
      "Cross-run stability. MCQ w/o refusal ranges from 89.0\u201391.5% across files, indicating reproducible evaluation."
    ],
    "weaknesses": [
      "bix-32-q2 (0/11): Complete failure across all repeats and all grading modes. This is not a grading issue \u2014 the model genuinely cannot answer this question.",
      "bix-16 task group (39.4%): The weakest multi-question task group. Two of its three questions (q1 and q3) fail at 9.1%, while q4 scores 100%.",
      "bix-24-q2 (63.6%): Moderate failure rate with no MCQ lift \u2014 the model has a genuine knowledge gap here.",
      "Answer precision issues: 11 questions show a refusal gap, meaning the model gets to the right answer but not precisely enough.",
      "Run-to-run variance: While the average is stable, individual repeats range from 82.0% to 96.0% \u2014 a 14.0pp spread."
    ],
    "key_takeaways": [
      "The model's true knowledge level is closer to 90.0% than 71.3%. The 18.7pp gap between direct (71.3%) and MCQ (90.0%) is largely a grading artifact.",
      "3 questions account for 56.4% of all MCQ failures. bix-32-q2, bix-16-q3, and bix-16-q1 together account for 31 of the 55 total MCQ w/o refusal failures. Fixing these three would push the score to approximately 95.6%.",
      "The 4.9pp refusal gap (90.0% \u2192 85.1%) represents answer precision, not knowledge.",
      "Best single-repeat score is 96.0% (48/50). This demonstrates the model's ceiling \u2014 it's capable of near-perfect performance in favorable conditions.",
      "The benchmark has good discriminative power. With 38 always-correct, 1 always-wrong, and 11 mixed questions, the benchmark effectively separates reliable knowledge from uncertain areas."
    ]
  },
  "latency": {
    "mean_ms": 907778.6,
    "p95_ms": 1702785.3
  },
  "assets": {
    "performance_plot": "assets/performance_analysis.png",
    "csv_dir": "data/results"
  },
  "notes": {
    "scoped_to_source_files": true,
    "source_markdown": "results/saved_output.md"
  }
}